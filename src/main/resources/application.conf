# This is the main configuration file for our application, it provides overrides to the default values
# provided in the reference.conf of the modules from Akka
akka {
  actor {
    # Must be set like this to use Akka Cluster
    provider = cluster

    # Only for convenience in the quickstart, Java serialization should not be used for actual applications
    warn-about-java-serializer-usage = off

    deployment {
      user/query-master/singleton/userRepoRouter {
        router = round-robin-pool
        cluster {
          enabled = on
          allow-local-routees = off
          max-nr-of-instances-per-node = 1
          max-total-nr-of-instances = 10000
          use-role = query-worker
        }
      }
      user/query-master/singleton/insuranceServiceRouter {
        router = round-robin-pool
        cluster {
          enabled = on
          allow-local-routees = off
          max-nr-of-instances-per-node = 1
          max-total-nr-of-instances = 10000
          use-role = query-worker
        }
      }
      user/command-master/singleton/workerRouter {
        router = round-robin-pool
        cluster {
          enabled = on
          allow-local-routees = off
          max-nr-of-instances-per-node = 1
          max-total-nr-of-instances = 10000
          use-role = command-worker
        }
      }
    }
  }

  # Use slf4j (backed by logback) for logging, additional configuration
  # can be done in logback.xml
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = DEBUG

  # For the sample, just bind to loopback and do not allow access from the network
  remote.netty.tcp.hostname=127.0.0.1
  # the port is overridden by the logic in Main.scala
  remote.netty.tcp.port=0

  cluster {
//    log-info = off
    # Seed nodes are a way to have a node join the cluster (or form a new cluster) from configuration.
    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552",
      "akka.tcp://ClusterSystem@127.0.0.1:2553",
      "akka.tcp://ClusterSystem@127.0.0.1:2554",
      "akka.tcp://ClusterSystem@127.0.0.1:2561",
      "akka.tcp://ClusterSystem@127.0.0.1:2562",
      "akka.tcp://ClusterSystem@127.0.0.1:2563"
    ]

    # Only for convenience in the quickstart, auto-downing should not be used for actual applications.
    # Read more here: http://doc.akka.io/docs/akka/current/scala/cluster-usage.html#auto-downing-do-not-use-
    auto-down-unreachable-after = 10s

    # Needed when running many actor systems in the same JVM
    jmx.multi-mbeans-in-same-jvm = on
  }

  # use Cassandra to store both snapshots and the events of the persistent actors
  persistence {
    fsm.snapshot-after = 1000
    journal.plugin = "cassandra-journal"
    snapshot-store.plugin = "cassandra-snapshot-store"
  }

  # Run the pubsub mediator on all nodes, without any code starting it up
//  extensions = ["akka.cluster.pubsub.DistributedPubSub"]
}

# Configuration related to the app is in its own namespace
distributed-workers {

  # If a workload hasn't finished in this long it
  # is considered failed and is retried
  work-timeout = 15s
}

http-rest-server {

  http-host = localhost

  http-port = 8080

  timeout = 5s
}